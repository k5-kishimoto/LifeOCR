import os
import json
import io
import time
import re
import ast
import concurrent.futures
from pdf2image import convert_from_bytes
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from PIL import Image, ImageEnhance, ImageOps 
from dotenv import load_dotenv

load_dotenv()

class OcrEngine:
    def __init__(self):
        """ÂàùÊúüÂåñ"""
        self.api_key = os.environ.get("GEMINI_API_KEY")
        self.model = None
        
        if not self.api_key:
            print("‚ùå Error: 'GEMINI_API_KEY' not found.")
            return

        try:
            genai.configure(api_key=self.api_key)
            self.model_name = os.environ.get("GEMINI_VERSION", "gemini-2.5-flash")
            
            self.generation_config = genai.types.GenerationConfig(
                temperature=0.3, 
                top_p=0.95,
                max_output_tokens=8192,
                response_mime_type="application/json"
            )

            self.safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
            
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            print(f"‚öôÔ∏è Initial Model config: {self.model_name} (Merge-Update Mode)")

        except Exception as e:
            print(f"‚ùå API Configuration Error: {e}")

    # =========================================================================
    # üßπ „ÇØ„É™„Éº„Éã„É≥„Ç∞ & Âà§ÂÆö
    # =========================================================================
    
    def _clean_text(self, val):
        if val is None: return ""
        if isinstance(val, (dict, list)): val = str(val)
        val = str(val)
        val = val.replace("\n", "").replace("\r", "")
        val = val.replace("‚ñ†", " ") 
        val = re.sub(r'\s+', ' ', val)
        return val.strip()

    def _is_header_row(self, row):
        """„Éò„ÉÉ„ÉÄ„ÉºË°åÂà§ÂÆöÔºàBottomÂÅ¥„ÅÆ‰∏çË¶Å„Å™„Éò„ÉÉ„ÉÄ„ÉºÈô§ÂéªÁî®Ôºâ"""
        header_keywords = ["Êó•‰ªò", "ÊëòË¶Å", "ÈáëÈ°ç", "ÂÖ•Èáë", "Âá∫Èáë", "ÊÆãÈ´ò", "ÂÄüÊñπ", "Ë≤∏Êñπ", "Âå∫ÂàÜ", "ÊîØÂ∫óÂêç"]
        match_count = 0
        for cell in row:
            text = str(cell)
            if any(k in text for k in header_keywords):
                match_count += 1
        return match_count >= 2

    def _is_same_transaction(self, row1, row2):
        """
        2„Å§„ÅÆË°å„Åå„ÄåÂêå„ÅòÂèñÂºï„Äç„ÅãÂà§ÂÆö„Åô„Çã„ÄÇ
        Êó•‰ªò„Å®ÈáëÈ°ç„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å¶„ÄÅ„Åù„Çå„Åå‰∏ÄËá¥„Åô„Çå„Å∞Âêå‰∏Ä„Å®„Åø„Å™„Åô„ÄÇ
        """
        # „ÉÜ„Ç≠„Çπ„ÉàÊäΩÂá∫
        r1_text = "".join([self._clean_text(c) for c in row1])
        r2_text = "".join([self._clean_text(c) for c in row2])

        # Êó•‰ªòÊäΩÂá∫ (YYYY/MM/DD)
        date1 = re.search(r'\d{4}[/-Âπ¥]\d{1,2}[/-Êúà]\d{1,2}', r1_text)
        date2 = re.search(r'\d{4}[/-Âπ¥]\d{1,2}[/-Êúà]\d{1,2}', r2_text)
        
        # ÈáëÈ°çÊäΩÂá∫ (3Ê°Å‰ª•‰∏ä„ÅÆÊï∞Â≠ó)
        amt1 = re.search(r'\d{1,3}(,\d{3})+', r1_text)
        amt2 = re.search(r'\d{1,3}(,\d{3})+', r2_text)

        # Âà§ÂÆö
        if date1 and date2 and date1.group() == date2.group():
            if amt1 and amt2 and amt1.group() == amt2.group():
                return True
        return False

    # =========================================================================
    # üñºÔ∏è ÁîªÂÉèÂá¶ÁêÜ
    # =========================================================================

    def _optimize_image(self, img):
        max_size = 2560 
        if max(img.size) > max_size:
            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
        if img.mode != 'RGB':
            img = img.convert('RGB')
        img = ImageOps.autocontrast(img, cutoff=2)
        enhancer = ImageEnhance.Sharpness(img)
        img = enhancer.enhance(1.5) 
        return img

    def _split_image(self, img):
        width, height = img.size
        split_ratio = 0.60
        overlap = 0.40
        crop_top = img.crop((0, 0, width, int(height * split_ratio)))
        crop_bottom = img.crop((0, int(height * overlap), width, height))
        return [("Top", crop_top), ("Bottom", crop_bottom)]

    # =========================================================================
    # üß† „Éá„Éº„ÇøËß£Êûê
    # =========================================================================

    def _repair_json(self, text):
        if not text: return None
        try:
            cleaned = text.strip()
            if cleaned.startswith("```json"): cleaned = cleaned[7:-3]
            elif cleaned.startswith("```"): cleaned = cleaned[3:-3]
            return json.loads(cleaned)
        except: pass
        try:
            if cleaned.count('"') % 2 != 0: cleaned += '"'
            if not cleaned.endswith("}"): cleaned += "}]}"
            return json.loads(cleaned)
        except: pass
        try:
            candidate_rows = re.findall(r'\[(.*?)\]', text, re.DOTALL)
            valid_rows = []
            for row_content in candidate_rows:
                if not row_content.strip(): continue
                try:
                    row_data = json.loads(f"[{row_content}]")
                    if isinstance(row_data, list): valid_rows.append(row_data)
                    continue
                except: pass
                try:
                    row_data = ast.literal_eval(f"[{row_content}]")
                    if isinstance(row_data, list): valid_rows.append(row_data)
                    continue
                except: pass
                try:
                    items = re.findall(r'"([^"]*)"', row_content)
                    if items: valid_rows.append(items)
                except: pass
            if valid_rows: return {"table_rows": valid_rows}
        except: pass
        return None

    def _call_ai_api(self, image_part, part_label):
        prompt = """
        „ÅÇ„Å™„Åü„ÅØÊó•Êú¨Ë™ûOCR„Ç®„É≥„Ç∏„É≥„Åß„Åô„ÄÇÁîªÂÉè„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        
        „ÄêÈáçË¶ÅÂëΩ‰ª§„Äë
        - **ÊîπË°å„Ç≥„Éº„ÉâÁ¶ÅÊ≠¢„ÄÇ1Ë°å„Å´„Å§„Å™„Åí„Çã„ÄÇ**
        - **„ÅÇ„Çã„Åå„Åæ„ÅæÊäΩÂá∫„Åô„Çã„Åì„Å®„ÄÇ**
        - Á©∫Ê¨ÑÁ¶ÅÊ≠¢„ÄÇÊé®Ê∏¨„Åó„Å¶Âüã„ÇÅ„Çã„ÄÇ
        - ÂçäËßí„Ç´„Éä„ÅØÂçäËßí„ÅÆ„Åæ„Åæ„ÄÇ
        - ÈÄî‰∏≠„Å´„ÅÇ„Çã„Éò„ÉÉ„ÉÄ„ÉºË°å„ÅØÁÑ°Ë¶ñ„Åó„Å¶„Éá„Éº„ÇøË°å„Å†„ÅëÊäΩÂá∫„ÄÇ

        „ÄêÂá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„Éà (JSON)„Äë
        {
          "document_info": { "title": "„Çø„Ç§„Éà„É´", "org_name": "Áô∫Ë°åÂÖÉ", "sub_name": "ÊîØÂ∫ó", "account_name": "ÂêçÁæ©", "period": "ÊúüÈñì", "other_info": "„Åù„ÅÆ‰ªñ" },
          "table_headers": ["È†ÖÁõÆ1", "È†ÖÁõÆ2", ...],
          "table_rows": [ 
             ["2026-01-22", "ÔæåÔæòÔΩ∫Ôæê ÔæÉÔΩΩÔæÑ", "10,000", "", "50,000", "Êú¨Â∫ó"],
          ]
        }
        """

        retry_models = [self.model_name, 'gemini-2.5-pro', 'gemini-2.0-flash']
        
        for current_model_name in retry_models:
            try:
                current_model = genai.GenerativeModel(
                    current_model_name,
                    generation_config=self.generation_config,
                    safety_settings=self.safety_settings
                )
                response = current_model.generate_content([prompt, image_part])
                try:
                    if not response.candidates: raise ValueError("No candidates")
                    return response.text
                except ValueError as ve:
                    if response.candidates and response.candidates[0].content.parts:
                        return response.candidates[0].content.parts[0].text
                    raise ve
            except Exception as e:
                print(f"‚ö†Ô∏è API Error ({part_label} - {current_model_name}): {e}")
                time.sleep(1)
                continue
        return None

    # =========================================================================
    # üîÑ ÂêàÊàê„Éû„Éº„Ç∏Ôºà‰∏äÊõ∏„ÅçÊõ¥Êñ∞„É≠„Ç∏„ÉÉ„ÇØÔºâ
    # =========================================================================

    def _merge_split_results(self, results):
        combined_json = { "document_info": {}, "table_headers": [], "table_rows": [] }

        # TopÊÉÖÂ†±„ÇíÂÑ™ÂÖà
        target_source = "Top" if "Top" in results else "Bottom"
        if target_source in results:
            combined_json["document_info"] = results[target_source].get("document_info", {})
            combined_json["table_headers"] = results[target_source].get("table_headers", [])

        # --- „Éû„Éº„Ç∏Âá¶ÁêÜ ---
        final_rows = []
        
        # 1. Top„ÅÆË°å„Çí„Åô„Åπ„Å¶ËøΩÂä†
        top_rows = results.get("Top", {}).get("table_rows", [])
        for row in top_rows:
            if not row or all(str(c).strip() == "" for c in row): continue
            if self._is_header_row(row): continue
            
            cleaned = [self._clean_text(c) for c in row]
            final_rows.append(cleaned)

        # 2. Bottom„ÅÆË°å„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        bottom_rows = results.get("Bottom", {}).get("table_rows", [])
        
        for b_row in bottom_rows:
            if not b_row or all(str(c).strip() == "" for c in b_row): continue
            if self._is_header_row(b_row): continue

            b_cleaned = [self._clean_text(c) for c in b_row]
            
            # Êó¢Â≠ò„ÅÆË°å„Å®ÈáçË§á„Åó„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            matched_index = -1
            
            # Bottom„ÅØ‰∏ãÂçäÂàÜ„Å™„ÅÆ„Åß„ÄÅTop„ÅÆ„ÄåÂæå„Çç„ÅÆÊñπ„Äç„Å®ÈáçË§á„Åô„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ
            # Âæå„Çç„Åã„ÇâÈ†Ü„Å´Êé¢„Åô„Åì„Å®„ÅßÂäπÁéáÂåñÔºÜË™§ÁàÜÈò≤Ê≠¢
            search_range = range(len(final_rows) - 1, max(-1, len(final_rows) - 10), -1)
            
            for i in search_range:
                t_row = final_rows[i]
                if self._is_same_transaction(t_row, b_cleaned):
                    matched_index = i
                    break
            
            if matched_index != -1:
                # ‚òÖÈáçË¶ÅÂ§âÊõ¥ÁÇπ: ÈáçË§á„ÅåË¶ã„Å§„Åã„Å£„Åü„Çâ„ÄÅ„Çà„ÇäÊÉÖÂ†±Èáè„ÅåÂ§ö„ÅÑ„Çª„É´„Åß„Äå‰∏äÊõ∏„Åç„Äç„Åô„Çã
                existing_row = final_rows[matched_index]
                merged_row = []
                
                # Èï∑„ÅÑÊñπ„ÇíÊé°Áî®„Åó„Å¶ÂêàÊàê„Åô„ÇãÔºà„Çª„É´Âçò‰Ωç„ÅÆ„Éû„Éº„Ç∏Ôºâ
                # ‰æã: Top["", "100"] + Bottom["ÊëòË¶Å„ÅÇ„Çä", "100"] -> Result["ÊëòË¶Å„ÅÇ„Çä", "100"]
                max_cols = max(len(existing_row), len(b_cleaned))
                
                for i in range(max_cols):
                    val_t = existing_row[i] if i < len(existing_row) else ""
                    val_b = b_cleaned[i] if i < len(b_cleaned) else ""
                    
                    # ÊñáÂ≠óÊï∞„ÅåÈï∑„ÅÑÊñπ„ÇíÊé°Áî®ÔºàÊÉÖÂ†±Èáè„ÅåÂ§ö„ÅÑ„Å®„Åø„Å™„ÅôÔºâ
                    if len(val_b) > len(val_t):
                        merged_row.append(val_b)
                    else:
                        merged_row.append(val_t)
                
                # Êõ¥Êñ∞
                final_rows[matched_index] = merged_row
            else:
                # Êñ∞Ë¶èË°å„Å™„ÇâËøΩÂä†
                final_rows.append(b_cleaned)

        combined_json["table_rows"] = final_rows
        return combined_json, len(final_rows)

    def _format_to_ui_data(self, combined_json):
        formatted_rows = []

        # 1. ÊñáÊõ∏ÊÉÖÂ†±
        doc_info = combined_json.get("document_info", {})
        title_text = self._clean_text(doc_info.get('title'))
        if title_text: formatted_rows.append([{'text': f"‚ñ† {title_text}"}])
        
        org_info = []
        for key in ['org_name', 'sub_name', 'bank_name', 'branch_name']:
            val = self._clean_text(doc_info.get(key))
            if val: org_info.append(val)
        
        if org_info: formatted_rows.append([{'text': " ".join(org_info)}])

        meta_texts = []
        if doc_info.get("account_name"): meta_texts.append(f"ÂêçÁæ©: {self._clean_text(doc_info['account_name'])}")
        if doc_info.get("period"): meta_texts.append(f"ÊúüÈñì: {self._clean_text(doc_info['period'])}")
        if doc_info.get("other_info"): meta_texts.append(self._clean_text(doc_info['other_info']))
        if meta_texts: formatted_rows.append([{'text': " / ".join(meta_texts)}])
        
        formatted_rows.append([{'text': ""}])

        # 2. „Éò„ÉÉ„ÉÄ„Éº
        headers = combined_json.get("table_headers", [])
        if headers:
            clean_headers = [self._clean_text(h) for h in headers]
            formatted_rows.append([{'text': h} for h in clean_headers])

        # 3. ÊòéÁ¥∞„Éá„Éº„Çø
        for row in combined_json.get("table_rows", []):
            formatted_cells = [{'text': self._clean_text(cell)} for cell in row]
            formatted_rows.append(formatted_cells)

        return formatted_rows

    # =========================================================================
    # üöÄ „É°„Ç§„É≥Âá¶ÁêÜ
    # =========================================================================

    def _process_single_page(self, args):
        page_label, pil_image = args
        optimized_image = self._optimize_image(pil_image)
        parts = self._split_image(optimized_image)
        
        results = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_part = {}
            for p_name, p_img in parts:
                img_byte_arr = io.BytesIO()
                p_img.save(img_byte_arr, format='WEBP', quality=100)
                image_part = {"mime_type": "image/webp", "data": img_byte_arr.getvalue()}
                
                future = executor.submit(self._call_ai_api, image_part, f"{page_label}-{p_name}")
                future_to_part[future] = p_name

            for future in concurrent.futures.as_completed(future_to_part):
                p_name = future_to_part[future]
                res_text = future.result()
                if res_text:
                    repaired_data = self._repair_json(res_text)
                    if repaired_data:
                        results[p_name] = repaired_data
                    else:
                        print(f"‚ùå JSON Repair Failed for {p_name}")

        combined_json, row_count = self._merge_split_results(results)
        formatted_rows = self._format_to_ui_data(combined_json)
        
        print(f"‚úÖ Success ({page_label}) - Merged {row_count} rows")
        return (page_label, formatted_rows)


    def extract_text(self, uploaded_file):
        print(f"‚è≥ Starting Gemini AI OCR ({self.model_name}) - Merge-Update Mode...")
        if not self.model: return [[{'text': "Error: AI Model not initialized."}]]

        uploaded_file.seek(0)
        file_bytes = uploaded_file.read()
        
        try: filename = uploaded_file.name.lower()
        except: filename = "unknown.jpg"
            
        images_to_process = [] 
        if filename.endswith('.pdf'):
            try:
                pil_images = convert_from_bytes(file_bytes, dpi=250, fmt='jpeg')
                for i, img in enumerate(pil_images):
                    images_to_process.append((f"Page {i+1}", img))
            except Exception as e: return [[{'text': f"PDF Error: {e}"}]]
        else:
            img = Image.open(io.BytesIO(file_bytes))
            images_to_process.append(("Page 1", img))

        final_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_page = {executor.submit(self._process_single_page, item): item[0] for item in images_to_process}
            results_dict = {}
            for future in concurrent.futures.as_completed(future_to_page):
                page_label, page_data = future.result()
                results_dict[page_label] = page_data

        for label, _ in images_to_process:
            if len(images_to_process) > 1:
                final_results.append([{'text': f'--- {label} ---'}])
            if label in results_dict:
                final_results.extend(results_dict[label])
        return final_results

engine = OcrEngine()